{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from umc_module import main_program # import self define module\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df_raw=pd.read_excel('dataset_Nov.xlsx')\n",
    "df_raw.replace([False,True],[0,1],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['created_at'] = pd.to_datetime(df_raw['created_at'], '%Y-%m-%d %H:%M:%S')\n",
    "df_raw['created_at'].min(), df_raw['created_at'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng=df_raw.loc[df_raw.dataset=='english'].reset_index(drop=True)\n",
    "df_fre=df_raw.loc[df_raw.dataset=='french'].reset_index(drop=True)\n",
    "df_dut=df_raw.loc[df_raw.dataset=='dutch'].reset_index(drop=True)\n",
    "df_ita=df_raw.loc[df_raw.dataset=='italian'].reset_index(drop=True)\n",
    "df_fre.shape, df_dut.shape, df_ita.shape, df_eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng['created_at'].min(),df_eng['created_at'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fre['created_at'].min(),df_fre['created_at'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dut['created_at'].min(),df_dut['created_at'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ita['created_at'].min(),df_ita['created_at'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupvariable = 'tweet_has_disinfo_hashtags'\n",
    "featurelist = ['causal_words', 'cognitive_words', 'has_initial_conjunction',\\\n",
    "     'conjunctions', 'other_conjunctions', 'levelers', 'negations',\\\n",
    "         'refs_to_self','sense_words', 'structure', 'use']\n",
    "acc_dut = [0.8, 0.73, 1.0, 0.97, 0.91, 0.79, 0.88, 1.0, 1.0, 0.95, 0.77, 0.72]\n",
    "acc_eng = [0.8, 1.0, 1.0, 0.94, 0.9, 0.9, 0.9, 0.95, 1.0, 1.0, 0.8, 0.78]\n",
    "acc_fre = [0.8, 0.8, 1.0, 1.0, 0.91, 0.89, 0.87, 0.93, 0.92, 0.96, 0.9, 0.87]\n",
    "acc_ita = [0.8, 0.86, 1.0, 0.9, 0.84, 0.8, 1.0, 0.97, 0.98, 0.97, 0.74, 0.84]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\lct\\LEI\\2022-2023 S1\\Statistical consulting\\analysis_program\\umc_module\\main_program.py:54: RuntimeWarning: invalid value encountered in sqrt\n",
      "  se1 = np.sqrt((sum(x**2)/N- (1-p_hat)*z/(N*V_p)) - mu1**2)\n",
      "d:\\lct\\LEI\\2022-2023 S1\\Statistical consulting\\analysis_program\\umc_module\\main_program.py:54: RuntimeWarning: invalid value encountered in sqrt\n",
      "  se1 = np.sqrt((sum(x**2)/N- (1-p_hat)*z/(N*V_p)) - mu1**2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>mean1</th>\n",
       "      <th>se1</th>\n",
       "      <th>mean2</th>\n",
       "      <th>se2</th>\n",
       "      <th>p-value</th>\n",
       "      <th>Reject/Not</th>\n",
       "      <th>effect_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>causal_words</td>\n",
       "      <td>-0.005473</td>\n",
       "      <td>0.255589</td>\n",
       "      <td>0.039283</td>\n",
       "      <td>0.194267</td>\n",
       "      <td>9.722292e-04</td>\n",
       "      <td>Reject null hypothesis</td>\n",
       "      <td>Proportion should not less than 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cognitive_words</td>\n",
       "      <td>0.031852</td>\n",
       "      <td>0.23243</td>\n",
       "      <td>0.046503</td>\n",
       "      <td>0.210572</td>\n",
       "      <td>6.089784e-01</td>\n",
       "      <td>Accept null hypothesis</td>\n",
       "      <td>-0.075839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>has_initial_conjunction</td>\n",
       "      <td>0.106876</td>\n",
       "      <td>0.358213</td>\n",
       "      <td>0.16328</td>\n",
       "      <td>0.283231</td>\n",
       "      <td>1.306995e-02</td>\n",
       "      <td>Reject null hypothesis</td>\n",
       "      <td>-0.165863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conjunctions</td>\n",
       "      <td>0.593546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.379729</td>\n",
       "      <td>0.38149</td>\n",
       "      <td>1.243450e-14</td>\n",
       "      <td>Reject null hypothesis</td>\n",
       "      <td>0.431126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other_conjunctions</td>\n",
       "      <td>0.545012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.219084</td>\n",
       "      <td>0.284756</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>Reject null hypothesis</td>\n",
       "      <td>0.686744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>levelers</td>\n",
       "      <td>0.112591</td>\n",
       "      <td>0.177155</td>\n",
       "      <td>0.126158</td>\n",
       "      <td>0.142274</td>\n",
       "      <td>4.489894e-01</td>\n",
       "      <td>Accept null hypothesis</td>\n",
       "      <td>-0.041858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>negations</td>\n",
       "      <td>0.180919</td>\n",
       "      <td>0.408707</td>\n",
       "      <td>0.222847</td>\n",
       "      <td>0.354523</td>\n",
       "      <td>3.379178e-01</td>\n",
       "      <td>Accept null hypothesis</td>\n",
       "      <td>-0.104581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>refs_to_self</td>\n",
       "      <td>0.110665</td>\n",
       "      <td>0.626469</td>\n",
       "      <td>0.29647</td>\n",
       "      <td>0.456701</td>\n",
       "      <td>5.141289e-09</td>\n",
       "      <td>Reject null hypothesis</td>\n",
       "      <td>-0.473309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sense_words</td>\n",
       "      <td>0.033358</td>\n",
       "      <td>0.208611</td>\n",
       "      <td>0.040481</td>\n",
       "      <td>0.197085</td>\n",
       "      <td>6.089784e-01</td>\n",
       "      <td>Accept null hypothesis</td>\n",
       "      <td>-0.03782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feature     mean1       se1     mean2       se2  \\\n",
       "0             causal_words -0.005473  0.255589  0.039283  0.194267   \n",
       "1          cognitive_words  0.031852   0.23243  0.046503  0.210572   \n",
       "2  has_initial_conjunction  0.106876  0.358213   0.16328  0.283231   \n",
       "3             conjunctions  0.593546       NaN  0.379729   0.38149   \n",
       "4       other_conjunctions  0.545012       NaN  0.219084  0.284756   \n",
       "5                 levelers  0.112591  0.177155  0.126158  0.142274   \n",
       "6                negations  0.180919  0.408707  0.222847  0.354523   \n",
       "7             refs_to_self  0.110665  0.626469   0.29647  0.456701   \n",
       "8              sense_words  0.033358  0.208611  0.040481  0.197085   \n",
       "\n",
       "        p-value              Reject/Not                        effect_size  \n",
       "0  9.722292e-04  Reject null hypothesis  Proportion should not less than 0  \n",
       "1  6.089784e-01  Accept null hypothesis                          -0.075839  \n",
       "2  1.306995e-02  Reject null hypothesis                          -0.165863  \n",
       "3  1.243450e-14  Reject null hypothesis                           0.431126  \n",
       "4  0.000000e+00  Reject null hypothesis                           0.686744  \n",
       "5  4.489894e-01  Accept null hypothesis                          -0.041858  \n",
       "6  3.379178e-01  Accept null hypothesis                          -0.104581  \n",
       "7  5.141289e-09  Reject null hypothesis                          -0.473309  \n",
       "8  6.089784e-01  Accept null hypothesis                           -0.03782  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose 'tweet_has_disinfo_text_or_hashtags' as group variable\n",
    "# English data set\n",
    "p_value=main_program.multiple_test(featurelist[0:9],groupvariable,df_eng,acc_eng)\n",
    "res_eng=main_program.multiple_estimates(featurelist[0:9],groupvariable,df_eng,acc_eng)\n",
    "res_eng['p-value']=main_program.holm(p_value, alpha=0.05)\n",
    "res_eng['Reject/Not']=main_program.holm(p_value, False, alpha=0.05)\n",
    "res_eng['effect_size'] = res_eng[['mean1', 'mean2']].apply(main_program.effect_size, axis=1)\n",
    "# mean1: information group; mean2: disinformation group\n",
    "# effect size = standardized (mean1 - mean2)\n",
    "res_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose 'tweet_has_disinfo_text_or_hashtags' as group variable\n",
    "# French data set\n",
    "p_value=main_program.multiple_test(featurelist[0:9],groupvariable,df_fre,acc_fre)\n",
    "res_fre=main_program.multiple_estimates(featurelist[0:9],groupvariable,df_fre,acc_fre)\n",
    "res_fre['p-value']=main_program.holm(p_value, alpha=0.05)\n",
    "res_fre['Reject/Not']=main_program.holm(p_value, p_value=False, alpha=0.05)\n",
    "res_fre['effect_size'] = res_fre[['mean1', 'mean2']].apply(main_program.effect_size, axis=1)\n",
    "res_fre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose 'tweet_has_disinfo_text_or_hashtags' as group variable\n",
    "# Italy data set\n",
    "p_value=main_program.multiple_test(featurelist[0:9],groupvariable,df_ita,acc_ita)\n",
    "res_ita=main_program.multiple_estimates(featurelist[0:9],groupvariable,df_ita,acc_ita)\n",
    "res_ita['p-value']=main_program.holm(p_value, alpha=0.05)\n",
    "res_ita['Reject/Not']=main_program.holm(p_value, p_value=False, alpha=0.05)\n",
    "res_ita['effect_size'] = res_ita[['mean1', 'mean2']].apply(main_program.effect_size, axis=1)\n",
    "res_ita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose 'tweet_has_disinfo_text_or_hashtags' as group variable\n",
    "# Dutch data set\n",
    "p_value=main_program.multiple_test(featurelist[0:9],groupvariable,df_dut,acc_dut)\n",
    "res_dut=main_program.multiple_estimates(featurelist[0:9],groupvariable,df_dut,acc_dut)\n",
    "res_dut['p-value']=main_program.holm(p_value, alpha=0.05)\n",
    "res_dut['Reject/Not']=main_program.holm(p_value, p_value=False, alpha=0.05)\n",
    "res_dut['effect_size'] = res_dut[['mean1', 'mean2']].apply(main_program.effect_size, axis=1)\n",
    "res_dut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export results into excel file\n",
    "res_dut.to_excel('dut.xlsx', sheet_name='sheet1', index=True)\n",
    "res_eng.to_excel('eng.xlsx', sheet_name='sheet1', index=True)\n",
    "res_fre.to_excel('fre.xlsx', sheet_name='sheet1', index=True)\n",
    "res_ita.to_excel('ita.xlsx', sheet_name='sheet1', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb62e2cdb21f1d3c93994851baec4fd6c9674aa2005ba49c8248e2510b2293cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
